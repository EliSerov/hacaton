# Архитектура

## Компоненты
### 1) Telegram Bot Service (`telegram-bot-service`)
- Интерфейс (чат) для пользователя.
- Держит состояние фильтров (author/date/topic) на пользователя.
- Выполняет RPC-вызовы в `rag-service` через RabbitMQ.
- Реализует базовую авторизацию пользователей (allowlist по Telegram user_id).

### 2) RAG Service (`rag-service`)
Один процесс, один инстанс LLM → один CUDA-контекст (при сборке llama.cpp с CUDA и `LLM_N_GPU_LAYERS > 0`).

Функции:
- эмбеддинг запроса;
- поиск ближайших фрагментов (chunks) в Qdrant с учётом фильтров;
- агрегация chunk → article;
- генерация аннотационного резюме с ссылками `[n]`;
- рекомендации похожих публикаций;
- генерация вопросов/теста.

### 3) Indexer Service (`indexer-service`)
Батч-индексация CSV:
- читает статьи (полный `content`);
- нормализует поля;
- режет на чанки;
- считает эмбеддинги;
- upsert в Qdrant.

## Хранилища/инфраструктура
- **RabbitMQ**: транспорт и RPC (бот ↔ rag).
- **Qdrant**: векторное хранилище чанков статей.

## Потоки данных

### Поиск (search)
1. Bot → RabbitMQ (routing_key=`search`): `{query, filters}`
2. rag-service:
   - embed(query)
   - Qdrant search (vector + payload filters)
   - агрегирует результаты до top-N статей
   - LLM summary с цитированием `[n]`
3. rag-service → Bot (RPC reply): `{summary, articles}`

### Рекомендации (recommend)
MVP-логика: расширенный поиск по тому же query → исключение уже показанных URL → выдача похожих статей.

### Тест (quiz)
MVP-логика: top-3 источника по query → LLM генерирует тест (в тексте summary) + ссылки `[n]`.

## Этичность
- LLM получает только выдержки из статей и список источников.
- В промпте закреплено правило: **не выдумывать** факты и всегда ссылаться на источники `[n]`.
- Источники в summary соотносятся с `articles[n-1]`.

## Расширяемость
- Языки: добавить новый embedding/LLM и поле `lang` в payload.
- Новые источники: добавить CSV/парсер и переиндексацию.
- PDF/другие форматы: подключить загрузчик → извлечение текста → тот же пайплайн чанкинга/эмбеддингов.
